{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\r\n",
    "                             [93, 88, 93],\r\n",
    "                             [89, 91, 90],\r\n",
    "                             [96, 98, 100],\r\n",
    "                             [73, 66, 70]])\r\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "전체 데이터의 양이 매우 많을 경우 메모리의 한계로 계산이 불가능하거나 매우 시간이 오래 걸릴 수 있음\r\n",
    "\r\n",
    "따라서 전체 데이터를 작은 단위로 나누어서 학습하는 개념이 필요함 -> 미니매치(Mini Batch)\r\n",
    "\r\n",
    "- 전체 데이터에 대해 경사하강법 수행 : 배치 경사 하강법\r\n",
    "- 미니 배치 단위로 경사하강법 수행 : 미니 배치 경사 하강법\r\n",
    "\r\n",
    "- 배치 경사 하강법 : 전체 데이터를 사용하므로 가중치 최적값에 수렴하는 과정이 안정적 But, 계산량이 많음\r\n",
    "- 미니 배치 경사 하강법 : 전체 데이터의 일부만 보고 수행하므로 최적값으로 수렴하는 과정에서 값을 조금 헤매지만 훈련속도가 빠름\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이터레이션 : 한번의 에포크 내에서 이루어지는 매개변수인 W,b의 업데이트 횟수"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from torch.utils.data import TensorDataset # 텐서데이터셋\r\n",
    "from torch.utils.data import DataLoader # 데이터로더"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \r\n",
    "                               [93,  88,  93], \r\n",
    "                               [89,  91,  90], \r\n",
    "                               [96,  98,  100],   \r\n",
    "                               [73,  66,  70]])  \r\n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "dataset = TensorDataset(x_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model = nn.Linear(3,1)\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "nb_epochs = 20\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "  for batch_idx, samples in enumerate(dataloader):\r\n",
    "    # print(batch_idx)\r\n",
    "    # print(samples)\r\n",
    "    x_train, y_train = samples\r\n",
    "    # H(x) 계산\r\n",
    "    prediction = model(x_train)\r\n",
    "\r\n",
    "    # cost 계산\r\n",
    "    cost = F.mse_loss(prediction, y_train)\r\n",
    "\r\n",
    "    # cost로 H(x) 계산\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\r\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\r\n",
    "        cost.item()\r\n",
    "        ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 42576.289062\n",
      "Epoch    0/20 Batch 2/3 Cost: 6454.604492\n",
      "Epoch    0/20 Batch 3/3 Cost: 2487.422852\n",
      "Epoch    1/20 Batch 1/3 Cost: 1386.991455\n",
      "Epoch    1/20 Batch 2/3 Cost: 179.841110\n",
      "Epoch    1/20 Batch 3/3 Cost: 135.126083\n",
      "Epoch    2/20 Batch 1/3 Cost: 51.648911\n",
      "Epoch    2/20 Batch 2/3 Cost: 1.005404\n",
      "Epoch    2/20 Batch 3/3 Cost: 4.784155\n",
      "Epoch    3/20 Batch 1/3 Cost: 7.037238\n",
      "Epoch    3/20 Batch 2/3 Cost: 1.980136\n",
      "Epoch    3/20 Batch 3/3 Cost: 0.624548\n",
      "Epoch    4/20 Batch 1/3 Cost: 0.957097\n",
      "Epoch    4/20 Batch 2/3 Cost: 5.692524\n",
      "Epoch    4/20 Batch 3/3 Cost: 1.282953\n",
      "Epoch    5/20 Batch 1/3 Cost: 4.985240\n",
      "Epoch    5/20 Batch 2/3 Cost: 2.018960\n",
      "Epoch    5/20 Batch 3/3 Cost: 0.102874\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.163075\n",
      "Epoch    6/20 Batch 2/3 Cost: 2.730186\n",
      "Epoch    6/20 Batch 3/3 Cost: 10.063501\n",
      "Epoch    7/20 Batch 1/3 Cost: 2.204616\n",
      "Epoch    7/20 Batch 2/3 Cost: 6.139984\n",
      "Epoch    7/20 Batch 3/3 Cost: 0.003699\n",
      "Epoch    8/20 Batch 1/3 Cost: 0.081239\n",
      "Epoch    8/20 Batch 2/3 Cost: 5.406272\n",
      "Epoch    8/20 Batch 3/3 Cost: 1.835393\n",
      "Epoch    9/20 Batch 1/3 Cost: 0.330317\n",
      "Epoch    9/20 Batch 2/3 Cost: 5.953272\n",
      "Epoch    9/20 Batch 3/3 Cost: 0.324734\n",
      "Epoch   10/20 Batch 1/3 Cost: 0.788777\n",
      "Epoch   10/20 Batch 2/3 Cost: 5.619043\n",
      "Epoch   10/20 Batch 3/3 Cost: 0.126999\n",
      "Epoch   11/20 Batch 1/3 Cost: 3.023116\n",
      "Epoch   11/20 Batch 2/3 Cost: 3.205511\n",
      "Epoch   11/20 Batch 3/3 Cost: 1.494478\n",
      "Epoch   12/20 Batch 1/3 Cost: 5.078930\n",
      "Epoch   12/20 Batch 2/3 Cost: 3.710024\n",
      "Epoch   12/20 Batch 3/3 Cost: 0.190913\n",
      "Epoch   13/20 Batch 1/3 Cost: 1.690379\n",
      "Epoch   13/20 Batch 2/3 Cost: 5.080407\n",
      "Epoch   13/20 Batch 3/3 Cost: 2.219071\n",
      "Epoch   14/20 Batch 1/3 Cost: 0.347190\n",
      "Epoch   14/20 Batch 2/3 Cost: 1.721079\n",
      "Epoch   14/20 Batch 3/3 Cost: 9.444161\n",
      "Epoch   15/20 Batch 1/3 Cost: 2.271321\n",
      "Epoch   15/20 Batch 2/3 Cost: 2.415435\n",
      "Epoch   15/20 Batch 3/3 Cost: 4.822412\n",
      "Epoch   16/20 Batch 1/3 Cost: 1.368390\n",
      "Epoch   16/20 Batch 2/3 Cost: 0.740601\n",
      "Epoch   16/20 Batch 3/3 Cost: 9.257524\n",
      "Epoch   17/20 Batch 1/3 Cost: 1.840104\n",
      "Epoch   17/20 Batch 2/3 Cost: 5.292771\n",
      "Epoch   17/20 Batch 3/3 Cost: 0.074476\n",
      "Epoch   18/20 Batch 1/3 Cost: 3.895218\n",
      "Epoch   18/20 Batch 2/3 Cost: 2.590015\n",
      "Epoch   18/20 Batch 3/3 Cost: 0.302419\n",
      "Epoch   19/20 Batch 1/3 Cost: 2.943026\n",
      "Epoch   19/20 Batch 2/3 Cost: 4.940149\n",
      "Epoch   19/20 Batch 3/3 Cost: 0.022875\n",
      "Epoch   20/20 Batch 1/3 Cost: 1.018876\n",
      "Epoch   20/20 Batch 2/3 Cost: 5.463554\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.085268\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\r\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \r\n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\r\n",
    "pred_y = model(new_var) \r\n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[149.5857]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "91fa4ce6aae8c6710e24d087068f91dd0bd45b1fb43fdf6e02b1c3fbf71d5aa7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}